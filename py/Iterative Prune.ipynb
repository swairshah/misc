{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lotery Ticket Hypothesis:\n",
    "\n",
    "Papers:\n",
    "1. The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\n",
    "    Jonathan Frankle & Michael Carbin. ICLR 2019\n",
    "    \n",
    "    code: https://github.com/google-research/lottery-ticket-hypothesis\n",
    "    \n",
    "2. Deconstructing Lottery Tickets:Zeros, Signs, and the Supermask\n",
    "     Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski. NeurIPS 2019.\n",
    "     \n",
    "3. One ticket to win them all: generalizing lottery ticketinitializations across datasets and optimizers\n",
    "      Ari S. Morcos, Haonan Yu, Michela Paganini, Yuandong Tian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### A randomly-initialized, dense neural network contains a subnet- work that is initialized such that—when trained in isolation—it can match the test accuracy of the original network after training for at most the same number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Pruning?\n",
    "\n",
    "Reduction in size of a network. \n",
    "\n",
    "\n",
    "* Which part of the network to remove? (filters/individual weight units/connections)  <img src=\"pruning.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "* What criteria to decide the removal?  (Gradients/Activation/Magnitued)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Prunning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Storage\n",
    "\n",
    "* Faster Inference (Sparse Ops support in hardware)\n",
    "\n",
    "* Faster Training (in fine-tuning)\n",
    "\n",
    "* Curiosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Procedure\n",
    "\n",
    "\n",
    "1. Train a network\n",
    "\n",
    "2. Prune it : remove filters/individual weight unites/connections/\n",
    "\n",
    "3. Fine tune the pruned network.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Difficult to train the pruned sparse network__\n",
    "\n",
    "__Up to a certain level of pruning, a randomly reinitialized network can match the accuracy of the original network. Past this point, winning tickets continue to match the performance of the original network when randomly reinitialized networks cannot__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from livelossplot import PlotLosses\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "        \n",
    "    train_loss /= len(test_loader.dataset)\n",
    "    train_acc = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def nnz(model):\n",
    "    nonzero = total = 0\n",
    "    for name, p in model.named_parameters():\n",
    "        tensor = p.data.cpu().numpy()\n",
    "        nz_count = np.count_nonzero(tensor)\n",
    "        total_params = np.prod(tensor.shape)\n",
    "        nonzero += nz_count\n",
    "        total += total_params\n",
    "    return (round((nonzero/total)*100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('/tmp/data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('/tmp/data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "model = Net()\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "#scheduler = StepLR(optimizer, step_size=1, gamma=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout2d(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 26, 26]             320\n",
      "            Conv2d-2           [-1, 64, 24, 24]          18,496\n",
      "         Dropout2d-3           [-1, 64, 12, 12]               0\n",
      "            Linear-4                  [-1, 128]       1,179,776\n",
      "         Dropout2d-5                  [-1, 128]               0\n",
      "            Linear-6                   [-1, 10]           1,290\n",
      "================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.52\n",
      "Params size (MB): 4.58\n",
      "Estimated Total Size (MB): 5.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(1, 28, 28), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Original Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state_dict = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2923249980926514 5.11\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = test(model, device, test_loader)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "for name, param in model.named_parameters(): \n",
    "    if 'weight' in name:\n",
    "        step = step + 1\n",
    "mask = [None]* step \n",
    "step = 0\n",
    "for name, param in model.named_parameters(): \n",
    "    if 'weight' in name:\n",
    "        tensor = param.data.cpu().numpy()\n",
    "        mask[step] = np.ones_like(tensor)\n",
    "        step = step + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:0.0075, test_loss:0.0365\n",
      "train_acc:97.5733, test_acc:98.7400\n"
     ]
    }
   ],
   "source": [
    "plotlosses = PlotLosses()\n",
    "for epoch in range(1, 4):\n",
    "    train_loss, train_acc = train(model, device, train_loader, optimizer, epoch)\n",
    "    test_loss, test_acc = test(model, device, test_loader)\n",
    "    #plotlosses.update({'train_loss' : train_loss, 'train_acc':train_acc, 'test_loss':test_loss, 'test_acc':test_acc})    \n",
    "    #plotlosses.send()\n",
    "    \n",
    "print(f'train_loss:{train_loss:.4f}, test_loss:{test_loss:.4f}')\n",
    "print(f'train_acc:{train_acc:.4f}, test_acc:{test_acc:.4f}')\n",
    "#scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnz(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune - Compute Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "percent = 20\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        tensor = param.data.cpu().numpy()\n",
    "        alive = tensor[np.nonzero(tensor)] \n",
    "        percentile_value = np.percentile(abs(alive), percent)\n",
    "\n",
    "        new_mask = np.where(abs(tensor) < percentile_value, 0, mask[step])\n",
    "\n",
    "        param.data = torch.from_numpy(tensor * new_mask).to(device)\n",
    "        mask[step] = new_mask\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune - Reset un-pruned to original init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "for name, param in model.named_parameters(): \n",
    "    if \"weight\" in name: \n",
    "        param.data = torch.from_numpy(mask[step] * initial_state_dict[name].cpu().numpy()).to(device)\n",
    "        step = step + 1\n",
    "    if \"bias\" in name:\n",
    "        param.data = initial_state_dict[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1275314865112303 71.84\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = test(model, device, test_loader)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnz(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:0.0171, test_loss:0.0536\n",
      "train_acc:94.7317, test_acc:98.2300\n"
     ]
    }
   ],
   "source": [
    "plotlosses = PlotLosses()\n",
    "for epoch in range(1, 2):\n",
    "    train_loss, train_acc = train(model, device, train_loader, optimizer, epoch)\n",
    "    test_loss, test_acc = test(model, device, test_loader)\n",
    "    \n",
    "print(f'train_loss:{train_loss:.4f}, test_loss:{test_loss:.4f}')\n",
    "print(f'train_acc:{train_acc:.4f}, test_acc:{test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the masked weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "for name, param in model.named_parameters(): \n",
    "    if \"weight\" in name: \n",
    "        param.data = torch.from_numpy(mask[step] * param.data.cpu().numpy()).to(device)\n",
    "        step = step + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09036706681251526 97.96\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = test(model, device, test_loader)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnz(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "percent = 20\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        tensor = param.data.cpu().numpy()\n",
    "        alive = tensor[np.nonzero(tensor)] \n",
    "        percentile_value = np.percentile(abs(alive), percent)\n",
    "\n",
    "        new_mask = np.where(abs(tensor) < percentile_value, 0, mask[step])\n",
    "\n",
    "        param.data = torch.from_numpy(tensor * new_mask).to(device)\n",
    "        mask[step] = new_mask\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnz(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting Questions\n",
    "\n",
    "* Is it over-fitting or does the lottery ticket generalize?\n",
    "\n",
    "* Does it work also for NLP/RL and other CV architectures?\n",
    "\n",
    "* Inference Speed-up?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra referenes based on the discussion after the talk\n",
    "\n",
    "* Lottery ticket for RL and NLP: \n",
    "    PLAYING THE LOTTERY WITH REWARDS ANDMULTIPLE LANGUAGES:LOTTERY TICKETS IN RL AND NLP Haonan Yu, Sergey Edunov, Yuandong Tian, and Ari S. Morcos\n",
    "    \n",
    "* Generalization on different datasets:\n",
    "    One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers, \n",
    "    Ari S. Morcos, Haonan Yu, Michela Paganini, Yuandong Tian\n",
    "    \n",
    "    \n",
    "* Theory paper justifying why pruning works (I haven't read it):\n",
    "    Proving the Lottery Ticket Hypothesis: Pruning is All You Need\n",
    "    Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, Ohad Shamir\n",
    "    \n",
    "    \n",
    "* Follow up work to assess different network distillation/pruning approaches:\n",
    "    WHAT IS THESTATE OF NEURAL NETWORK PRUNING? Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, John Guttag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
